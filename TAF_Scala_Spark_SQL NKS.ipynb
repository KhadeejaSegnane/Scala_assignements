{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data avec Spark : Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEGNANE Ndèye Khady"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problematique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet consiste à utiliser Apache Spark pour faire l'analyse et le traitement des données de **[San Francisco Fire Department Calls ](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)** afin de fournir quelques KPI (*Key Performance Indicator*). Le **SF Fire Datasets** comprend les réponses aux appels de toutes les unités d'incendie. Chaque enregistrement comprend le numéro d'appel, le numéro d'incident, l'adresse, l'identifiant de l'unité, le type d'appel et la disposition. Tous les intervalles de temps pertinents sont également inclus. Étant donné que ce Dataset est basé sur les réponses et que la plupart des appels impliquent plusieurs unités, ainsi il existe plusieurs enregistrements pour chaque numéro d'appel. Les adresses sont associées à un numéro de bloc, à une intersection ou à une boîte d'appel, et non à une adresse spécifique.\n",
    "\n",
    "**Plus de details sur la description des données [ici](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)**\n",
    "\n",
    "**Download csv file [here](https://data.sfgov.org/api/views/nuek-vuh3/rows.csv?accessType=DOWNLOAD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire.\n",
    "L'objectif de ce travail est de comprendre le Dataset SF Fire afin de bien répondre aux questions en utilisant les codes Spark/Scala adéquats.\n",
    "\n",
    "- Code lisible et bien indenté, \n",
    "- N'oublier pas de mettre en commentaire la justification de votre réponse sur les cellule Markdown. \n",
    "\n",
    "\n",
    "#### Note:\n",
    "- Vous pouvez en groupe (au plus deux étudiants) . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Importez les modules Spark necessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`sh.almond::almond-spark:0.10.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mrootLogger\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.spi.RootLogger@4401060d\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val rootLogger = Logger.getRootLogger()\n",
    "rootLogger.setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org.spark-project\").setLevel(Level.WARN)\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Creez la Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "23/04/26 14:50:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/26 14:50:18 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path\n",
      "java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n",
      "\tat org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)\n",
      "\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)\n",
      "\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)\n",
      "\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\n",
      "\tat org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)\n",
      "\tat org.apache.hadoop.security.Groups.<init>(Groups.java:93)\n",
      "\tat org.apache.hadoop.security.Groups.<init>(Groups.java:73)\n",
      "\tat org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n",
      "\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2422)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2422)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:293)\n",
      "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$5(SparkSession.scala:935)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)\n",
      "\tat ammonite.$sess.cmd2$Helper.<init>(cmd2.sc:5)\n",
      "\tat ammonite.$sess.cmd2$.<init>(cmd2.sc:7)\n",
      "\tat ammonite.$sess.cmd2$.<clinit>(cmd2.sc)\n",
      "\tat ammonite.$sess.cmd2.$main(cmd2.sc)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$evalMain$1(Evaluator.scala:108)\n",
      "\tat ammonite.util.Util$.withContextClassloader(Util.scala:24)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.evalMain(Evaluator.scala:90)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$2(Evaluator.scala:127)\n",
      "\tat ammonite.util.Catching.map(Res.scala:117)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$1(Evaluator.scala:121)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.processLine(Evaluator.scala:120)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$4(Interpreter.scala:295)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$2(Interpreter.scala:281)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.evaluateLine(Interpreter.scala:280)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$6(Interpreter.scala:268)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$4(Interpreter.scala:251)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$2(Interpreter.scala:244)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.processLine(Interpreter.scala:243)\n",
      "\tat almond.Execute.$anonfun$ammResult$11(Execute.scala:238)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$2(CaptureImpl.scala:53)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withErr(Console.scala:196)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$1(CaptureImpl.scala:45)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withOut(Console.scala:167)\n",
      "\tat almond.internals.CaptureImpl.apply(CaptureImpl.scala:45)\n",
      "\tat almond.Execute.capturingOutput(Execute.scala:166)\n",
      "\tat almond.Execute.$anonfun$ammResult$10(Execute.scala:225)\n",
      "\tat almond.Execute.$anonfun$withClientStdin$1(Execute.scala:146)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withIn(Console.scala:230)\n",
      "\tat almond.Execute.withClientStdin(Execute.scala:142)\n",
      "\tat almond.Execute.$anonfun$ammResult$9(Execute.scala:225)\n",
      "\tat almond.Execute.withInputManager(Execute.scala:134)\n",
      "\tat almond.Execute.$anonfun$ammResult$8(Execute.scala:224)\n",
      "\tat ammonite.repl.Signaller.apply(Signaller.scala:28)\n",
      "\tat almond.Execute.interruptible(Execute.scala:183)\n",
      "\tat almond.Execute.$anonfun$ammResult$7(Execute.scala:223)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat almond.Execute.$anonfun$ammResult$1(Execute.scala:214)\n",
      "\tat almond.Execute.withOutputHandler(Execute.scala:157)\n",
      "\tat almond.Execute.ammResult(Execute.scala:214)\n",
      "\tat almond.Execute.apply(Execute.scala:311)\n",
      "\tat almond.ScalaInterpreter.execute(ScalaInterpreter.scala:127)\n",
      "\tat almond.interpreter.InterpreterToIOInterpreter.$anonfun$execute$2(InterpreterToIOInterpreter.scala:69)\n",
      "\tat cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:366)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:387)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:330)\n",
      "\tat cats.effect.internals.IOShift$Tick.run(IOShift.scala:36)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@657d02e6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "val spark = {\n",
    "    SparkSession.builder()\n",
    "        .master(\"local\")\n",
    "        .appName(\"BD-FS FIRE\")\n",
    "        .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Chargez les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez le `fireSchema` definit dans la cellule suivante pour le chargement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\r\n",
       "\u001b[36mfireSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"IncidentNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"WatchDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallFinalDisposition\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"AvailableDtTm\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"City\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Zipcode\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Battalion\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StationArea\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Box\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"OriginalPriority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Priority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FinalPriority\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ALSUnit\"\u001b[39m, BooleanType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallTypeGroup\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"NumAlarms\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitSequenceInCallDispatch\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FirePreventionDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"SupervisorDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Neighborhood\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Location\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"RowID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Delay\"\u001b[39m, FloatType, true, {})\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "  StructField(\"UnitID\", StringType, true),\n",
    "  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "  StructField(\"CallType\", StringType, true),                  \n",
    "  StructField(\"CallDate\", StringType, true),      \n",
    "  StructField(\"WatchDate\", StringType, true),\n",
    "  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "  StructField(\"AvailableDtTm\", StringType, true),\n",
    "  StructField(\"Address\", StringType, true),       \n",
    "  StructField(\"City\", StringType, true),       \n",
    "  StructField(\"Zipcode\", IntegerType, true),       \n",
    "  StructField(\"Battalion\", StringType, true),                 \n",
    "  StructField(\"StationArea\", StringType, true),       \n",
    "  StructField(\"Box\", StringType, true),       \n",
    "  StructField(\"OriginalPriority\", StringType, true),       \n",
    "  StructField(\"Priority\", StringType, true),       \n",
    "  StructField(\"FinalPriority\", IntegerType, true),       \n",
    "  StructField(\"ALSUnit\", BooleanType, true),       \n",
    "  StructField(\"CallTypeGroup\", StringType, true),\n",
    "  StructField(\"NumAlarms\", IntegerType, true),\n",
    "  StructField(\"UnitType\", StringType, true),\n",
    "  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "  StructField(\"Neighborhood\", StringType, true),\n",
    "  StructField(\"Location\", StringType, true),\n",
    "  StructField(\"RowID\", StringType, true),\n",
    "  StructField(\"Delay\", FloatType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"sf-fire-calls.csv\"\u001b[39m\r\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"sf-fire-calls.csv\"\n",
    "val data = spark.read\n",
    "          .option(\"header\",\"true\")\n",
    "          .csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 14:51:01 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le nombre de ligne est 175296\n",
      "le nombre de colonne est 28\n"
     ]
    }
   ],
   "source": [
    "println(\"le nombre de ligne est \" +data.count())\n",
    "println(\"le nombre de colonne est \" +data.columns.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Mettez en cache les donnees chargees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 14:52:09 WARN CacheManager: Asked to cache already cached data.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]\r\n",
       "\u001b[36mres6_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache\n",
    "data.persist//(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise la mise en cache quand on effectue plusieurs actions sur le même DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Supprimez tous les appels de type `Medical Incident`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: appliquez la methode `.filter()` a la colonne `CallType` avec l'operateur `=!=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata_without_mi\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data_without_mi = data.filter(col(\"CallType\")=!= \"Medical Incident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Combien de types d'appels distincts ont été passés ?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|Nombre de type d'appel|\n",
      "+----------------------+\n",
      "|                    30|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.countDistinct\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct\n",
    "data.agg(countDistinct(\"CallType\") as \"Nombre de type d'appel\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Quels types d'appels  ont été passés au service d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|CallType                                    |\n",
      "+--------------------------------------------+\n",
      "|Elevator / Escalator Rescue                 |\n",
      "|Marine Fire                                 |\n",
      "|Aircraft Emergency                          |\n",
      "|Confined Space / Structure Collapse         |\n",
      "|Administrative                              |\n",
      "|Alarms                                      |\n",
      "|Odor (Strange / Unknown)                    |\n",
      "|Citizen Assist / Service Call               |\n",
      "|HazMat                                      |\n",
      "|Watercraft in Distress                      |\n",
      "|Explosion                                   |\n",
      "|Oil Spill                                   |\n",
      "|Vehicle Fire                                |\n",
      "|Suspicious Package                          |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|\n",
      "|Other                                       |\n",
      "|Outside Fire                                |\n",
      "|Traffic Collision                           |\n",
      "|Assist Police                               |\n",
      "|Gas Leak (Natural and LP Gases)             |\n",
      "|Water Rescue                                |\n",
      "|Electrical Hazard                           |\n",
      "|High Angle Rescue                           |\n",
      "|Structure Fire                              |\n",
      "|Industrial Accidents                        |\n",
      "|Medical Incident                            |\n",
      "|Mutual Aid / Assist Outside Agency          |\n",
      "|Fuel Spill                                  |\n",
      "|Smoke Investigation (Outside)               |\n",
      "|Train / Rail Incident                       |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"CallType\").distinct.show(30, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. Trouvez toutes les réponses ou les délais sont supérieurs à 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "1. Renommez la colonne `Delay` -> `ReponseDelayedinMins`\n",
    "2. Retournez un nouveau DataFrame\n",
    "3. Affichez tous les appels où le temps de réponse au site d'incendie a eu un retard de plus de 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]\r\n",
       "\u001b[36mres10_1\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = data.withColumnRenamed(\"Delay\",\"ReponseDelayedinMins\")\n",
    "df1.filter(col(\"ReponseDelayedinMins\") > 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Convertissez les colonnes dates en timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "* `CallDate` -> `IncidentDate`\n",
    "* `WatchDate` -> `OnWatchDate`\n",
    "* `AvailableDtTm` -> `AvailableDtTS`\n",
    "exemple code pour le cas de `CallDate`:\n",
    "`dataframe.withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df2 =( data\n",
    "                .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")\n",
    "                .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\")\n",
    "                .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy\")).drop(\"AvailableDtTm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`IncidentDate`' given input columns: [StationArea, IncidentNumber, Zipcode, CallNumber, Delay, CallDate, NumAlarms, FirePreventionDistrict, UnitSequenceInCallDispatch, CallType, RowID, SupervisorDistrict, Location, Priority, Battalion, UnitID, CallFinalDisposition, AvailableDtTm, ALSUnit, UnitType, CallTypeGroup, Box, City, FinalPriority, OriginalPriority, WatchDate, Neighborhood, Address];;\n'Project ['IncidentDate, 'OnWatchDate, 'AvailableDtTS]\n+- Relation[CallNumber#10,UnitID#11,IncidentNumber#12,CallType#13,CallDate#14,WatchDate#15,CallFinalDisposition#16,AvailableDtTm#17,Address#18,City#19,Zipcode#20,Battalion#21,StationArea#22,Box#23,OriginalPriority#24,Priority#25,FinalPriority#26,ALSUnit#27,CallTypeGroup#28,NumAlarms#29,UnitType#30,UnitSequenceInCallDispatch#31,FirePreventionDistrict#32,SupervisorDistrict#33,... 4 more fields] csv\n\u001b[39m\r\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m111\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\r\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m273\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\r\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\r\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m273\u001b[39m)\r\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m266\u001b[39m)\r\n  scala.collection.AbstractTraversable.map(\u001b[32mTraversable.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m201\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m58\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m56\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m48\u001b[39m)\r\n  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m78\u001b[39m)\r\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3412\u001b[39m)\r\n  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1341\u001b[39m)\r\n  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1358\u001b[39m)\r\n  ammonite.$sess.cmd12$Helper.<init>(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m1\u001b[39m)\r\n  ammonite.$sess.cmd12$.<init>(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd12$.<clinit>(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "data.select(\"IncidentDate\",\"OnWatchDate\",\"AvailableDtTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Quels sont les types d'appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------+\n",
      "|CallType                     |count |\n",
      "+-----------------------------+------+\n",
      "|Medical Incident             |113794|\n",
      "|Structure Fire               |23319 |\n",
      "|Alarms                       |19406 |\n",
      "|Traffic Collision            |7013  |\n",
      "|Citizen Assist / Service Call|2524  |\n",
      "+-----------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(\"CallType\")\n",
    ".count()\n",
    ".orderBy(col(\"count\").desc)\n",
    ".show(5, truncate =false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11. Quels sont les boites postales rencontrées dans les appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Zipcode|\n",
      "+-------+\n",
      "|  94124|\n",
      "|  94102|\n",
      "|  94115|\n",
      "|  94114|\n",
      "|  94110|\n",
      "|  94109|\n",
      "|  94121|\n",
      "|  94116|\n",
      "|  94118|\n",
      "|  94118|\n",
      "|  94133|\n",
      "|  94111|\n",
      "|  94132|\n",
      "|  94134|\n",
      "|  94118|\n",
      "|  94117|\n",
      "|  94103|\n",
      "|  94115|\n",
      "|  94109|\n",
      "|  94110|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//zipcode = code postale\n",
    "val df2 = df1.filter(col(\"CallType\") === \"Medical Incident\")\n",
    "    .select(\"Zipcode\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12. Quels sont les quartiers de San Francisco dont les codes postaux sont `94102` et `94103`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|        Neighborhood|Zipcode|\n",
      "+--------------------+-------+\n",
      "|         Mission Bay|  94103|\n",
      "|Financial Distric...|  94103|\n",
      "| Castro/Upper Market|  94103|\n",
      "|    Western Addition|  94102|\n",
      "|            Nob Hill|  94102|\n",
      "|     South of Market|  94103|\n",
      "|        Potrero Hill|  94103|\n",
      "|        Hayes Valley|  94103|\n",
      "|     South of Market|  94102|\n",
      "|          Tenderloin|  94102|\n",
      "|          Tenderloin|  94103|\n",
      "|             Mission|  94103|\n",
      "|Financial Distric...|  94102|\n",
      "|        Hayes Valley|  94102|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"Neighborhood\", \"Zipcode\")\n",
    ".where(col(\"Zipcode\")=== 94102 || col(\"Zipcode\")=== 94103)\n",
    ".distinct()\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13. Determinez le nombre total d'appels, ainsi que la moyenne, le minimum et le maximum du temps de réponse des appels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|ReponseDelayedinMins|\n",
      "+-------+--------------------+\n",
      "|  count|              175296|\n",
      "|   mean|  3.8923641541750413|\n",
      "| stddev|   9.378286170882717|\n",
      "|    min|         0.016666668|\n",
      "|    max|                99.9|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(\"ReponseDelayedinMins\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14. Combien d'années distinctes trouve t-on dans ce Dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Appliquer la fonction `year()` a la colonne `IncidentDate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`IncidentDate`' given input columns: [StationArea, IncidentNumber, Zipcode, CallNumber, Delay, CallDate, NumAlarms, FirePreventionDistrict, UnitSequenceInCallDispatch, CallType, RowID, SupervisorDistrict, Location, Priority, Battalion, UnitID, CallFinalDisposition, AvailableDtTm, ALSUnit, UnitType, CallTypeGroup, Box, City, FinalPriority, OriginalPriority, WatchDate, Neighborhood, Address];;\n'Project [year('IncidentDate) AS year(IncidentDate)#1486]\n+- Relation[CallNumber#10,UnitID#11,IncidentNumber#12,CallType#13,CallDate#14,WatchDate#15,CallFinalDisposition#16,AvailableDtTm#17,Address#18,City#19,Zipcode#20,Battalion#21,StationArea#22,Box#23,OriginalPriority#24,Priority#25,FinalPriority#26,ALSUnit#27,CallTypeGroup#28,NumAlarms#29,UnitType#30,UnitSequenceInCallDispatch#31,FirePreventionDistrict#32,SupervisorDistrict#33,... 4 more fields] csv\n\u001b[39m\r\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m111\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\r\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m273\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\r\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\r\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m273\u001b[39m)\r\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m266\u001b[39m)\r\n  scala.collection.AbstractTraversable.map(\u001b[32mTraversable.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m201\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m58\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m56\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m48\u001b[39m)\r\n  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m78\u001b[39m)\r\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3412\u001b[39m)\r\n  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1341\u001b[39m)\r\n  ammonite.$sess.cmd17$Helper.<init>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m2\u001b[39m)\r\n  ammonite.$sess.cmd17$.<init>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd17$.<clinit>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "data\n",
    "    .select(year(col(\"IncidentDate\")))\n",
    "    .distinct()\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15. Quelle semaine de l'année 2018 a eu le plus d'appels d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`IncidentDate`' given input columns: [StationArea, IncidentNumber, Zipcode, CallNumber, Delay, CallDate, NumAlarms, FirePreventionDistrict, UnitSequenceInCallDispatch, CallType, RowID, SupervisorDistrict, Location, Priority, Battalion, UnitID, CallFinalDisposition, AvailableDtTm, ALSUnit, UnitType, CallTypeGroup, Box, City, FinalPriority, OriginalPriority, WatchDate, Neighborhood, Address];;\n'Filter ((year('IncidentDate) = 2018) && calltype#13)\n+- Relation[CallNumber#10,UnitID#11,IncidentNumber#12,CallType#13,CallDate#14,WatchDate#15,CallFinalDisposition#16,AvailableDtTm#17,Address#18,City#19,Zipcode#20,Battalion#21,StationArea#22,Box#23,OriginalPriority#24,Priority#25,FinalPriority#26,ALSUnit#27,CallTypeGroup#28,NumAlarms#29,UnitType#30,UnitSequenceInCallDispatch#31,FirePreventionDistrict#32,SupervisorDistrict#33,... 4 more fields] csv\n\u001b[39m\r\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m111\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m201\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m58\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m56\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m48\u001b[39m)\r\n  org.apache.spark.sql.Dataset.<init>(\u001b[32mDataset.scala\u001b[39m:\u001b[32m176\u001b[39m)\r\n  org.apache.spark.sql.Dataset.<init>(\u001b[32mDataset.scala\u001b[39m:\u001b[32m182\u001b[39m)\r\n  org.apache.spark.sql.Dataset$.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m64\u001b[39m)\r\n  org.apache.spark.sql.Dataset.withTypedPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3417\u001b[39m)\r\n  org.apache.spark.sql.Dataset.filter(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1490\u001b[39m)\r\n  ammonite.$sess.cmd19$Helper.<init>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m2\u001b[39m)\r\n  ammonite.$sess.cmd19$.<init>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd19$.<clinit>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "data\n",
    "    .filter(year(col(\"IncidentDate\"))=== 2018 && col(\"calltype\"))\n",
    "    \"\"\".groupby(weekofyeear(col(\"IncidentDate\")))\n",
    "    .count()\n",
    "    .orderby(col(\"count\").desc)\n",
    "    .show(1)\n",
    "\n",
    "//data.filter(year(col(\"IncidentDate\")) === 2018 && col(\"CallType\"))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16. Quels sont les quartiers de San Francisco qui ont connu le pire temps de réponse en 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`IncidentDate`' given input columns: [StationArea, IncidentNumber, Zipcode, CallNumber, Delay, CallDate, NumAlarms, FirePreventionDistrict, UnitSequenceInCallDispatch, CallType, RowID, SupervisorDistrict, Location, Priority, Battalion, UnitID, CallFinalDisposition, AvailableDtTm, ALSUnit, UnitType, CallTypeGroup, Box, City, FinalPriority, OriginalPriority, WatchDate, Neighborhood, Address];;\n'Filter ((year('IncidentDate) = 2018) && 'District)\n+- Relation[CallNumber#10,UnitID#11,IncidentNumber#12,CallType#13,CallDate#14,WatchDate#15,CallFinalDisposition#16,AvailableDtTm#17,Address#18,City#19,Zipcode#20,Battalion#21,StationArea#22,Box#23,OriginalPriority#24,Priority#25,FinalPriority#26,ALSUnit#27,CallTypeGroup#28,NumAlarms#29,UnitType#30,UnitSequenceInCallDispatch#31,FirePreventionDistrict#32,SupervisorDistrict#33,... 4 more fields] csv\n\u001b[39m\r\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m111\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m280\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m328\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m326\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m277\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m69\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m186\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\r\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m126\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m86\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m201\u001b[39m)\r\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m105\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m58\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m56\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m48\u001b[39m)\r\n  org.apache.spark.sql.Dataset.<init>(\u001b[32mDataset.scala\u001b[39m:\u001b[32m176\u001b[39m)\r\n  org.apache.spark.sql.Dataset.<init>(\u001b[32mDataset.scala\u001b[39m:\u001b[32m182\u001b[39m)\r\n  org.apache.spark.sql.Dataset$.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m64\u001b[39m)\r\n  org.apache.spark.sql.Dataset.withTypedPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3417\u001b[39m)\r\n  org.apache.spark.sql.Dataset.filter(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1490\u001b[39m)\r\n  ammonite.$sess.cmd21$Helper.<init>(\u001b[32mcmd21.sc\u001b[39m:\u001b[32m2\u001b[39m)\r\n  ammonite.$sess.cmd21$.<init>(\u001b[32mcmd21.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd21$.<clinit>(\u001b[32mcmd21.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "data\n",
    "    .filter(year(col(\"IncidentDate\"))=== 2018 && col(\"District\"))\n",
    "   \"\"\" .groupby(weekofyeear(col(\"District\")))\n",
    "    .count()\n",
    "    .orderby(col(\"count\").desc)\n",
    "    .show(1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17. Stocker les données sous format de fichiers Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 15:04:34 INFO CodecConfig: Compression: SNAPPY\n",
      "23/04/26 15:04:34 INFO CodecConfig: Compression: SNAPPY\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Parquet page size to 1048576\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Dictionary is on\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Validation is off\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Page size checking is: estimated\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Min row count for page size check is: 100\n",
      "23/04/26 15:04:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000\n",
      "23/04/26 15:04:35 ERROR Executor: Exception in task 0.0 in stage 33.0 (TID 811)\n",
      "java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\firedataService_parquet\\files\\_temporary\\0\\_temporary\\attempt_20230426150434_0033_m_000000_811\\part-00000-0cb4d441-845a-4211-8ad3-b49a05ec763c-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "23/04/26 15:04:35 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 811, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\firedataService_parquet\\files\\_temporary\\0\\_temporary\\attempt_20230426150434_0033_m_000000_811\\part-00000-0cb4d441-845a-4211-8ad3-b49a05ec763c-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "23/04/26 15:04:35 ERROR TaskSetManager: Task 0 in stage 33.0 failed 1 times; aborting job\n",
      "23/04/26 15:04:35 ERROR FileFormatWriter: Aborting job eefc9b54-a541-4d50-aea4-f5165ba206f4.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 811, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\firedataService_parquet\\files\\_temporary\\0\\_temporary\\attempt_20230426150434_0033_m_000000_811\\part-00000-0cb4d441-845a-4211-8ad3-b49a05ec763c-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1879)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1878)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:170)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat ammonite.$sess.cmd20$Helper.<init>(cmd20.sc:1)\n",
      "\tat ammonite.$sess.cmd20$.<init>(cmd20.sc:7)\n",
      "\tat ammonite.$sess.cmd20$.<clinit>(cmd20.sc)\n",
      "\tat ammonite.$sess.cmd20.$main(cmd20.sc)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$evalMain$1(Evaluator.scala:108)\n",
      "\tat ammonite.util.Util$.withContextClassloader(Util.scala:24)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.evalMain(Evaluator.scala:90)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$2(Evaluator.scala:127)\n",
      "\tat ammonite.util.Catching.map(Res.scala:117)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$1(Evaluator.scala:121)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.processLine(Evaluator.scala:120)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$4(Interpreter.scala:295)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$2(Interpreter.scala:281)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.evaluateLine(Interpreter.scala:280)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$6(Interpreter.scala:268)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$4(Interpreter.scala:251)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$2(Interpreter.scala:244)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.processLine(Interpreter.scala:243)\n",
      "\tat almond.Execute.$anonfun$ammResult$11(Execute.scala:238)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$2(CaptureImpl.scala:53)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withErr(Console.scala:196)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$1(CaptureImpl.scala:45)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withOut(Console.scala:167)\n",
      "\tat almond.internals.CaptureImpl.apply(CaptureImpl.scala:45)\n",
      "\tat almond.Execute.capturingOutput(Execute.scala:166)\n",
      "\tat almond.Execute.$anonfun$ammResult$10(Execute.scala:225)\n",
      "\tat almond.Execute.$anonfun$withClientStdin$1(Execute.scala:146)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withIn(Console.scala:230)\n",
      "\tat almond.Execute.withClientStdin(Execute.scala:142)\n",
      "\tat almond.Execute.$anonfun$ammResult$9(Execute.scala:225)\n",
      "\tat almond.Execute.withInputManager(Execute.scala:134)\n",
      "\tat almond.Execute.$anonfun$ammResult$8(Execute.scala:224)\n",
      "\tat ammonite.repl.Signaller.apply(Signaller.scala:28)\n",
      "\tat almond.Execute.interruptible(Execute.scala:183)\n",
      "\tat almond.Execute.$anonfun$ammResult$7(Execute.scala:223)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat almond.Execute.$anonfun$ammResult$1(Execute.scala:214)\n",
      "\tat almond.Execute.withOutputHandler(Execute.scala:157)\n",
      "\tat almond.Execute.ammResult(Execute.scala:214)\n",
      "\tat almond.Execute.apply(Execute.scala:311)\n",
      "\tat almond.ScalaInterpreter.execute(ScalaInterpreter.scala:127)\n",
      "\tat almond.interpreter.InterpreterToIOInterpreter.$anonfun$execute$2(InterpreterToIOInterpreter.scala:69)\n",
      "\tat cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:366)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:387)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:330)\n",
      "\tat cats.effect.internals.IOShift$Tick.run(IOShift.scala:36)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\firedataService_parquet\\files\\_temporary\\0\\_temporary\\attempt_20230426150434_0033_m_000000_811\\part-00000-0cb4d441-845a-4211-8ad3-b49a05ec763c-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\t... 3 more\n",
      "23/04/26 15:04:35 WARN FileUtil: Failed to delete file or dir [C:\\tmp\\firedataService_parquet\\files\\_temporary\\0\\_temporary\\attempt_20230426150434_0033_m_000000_811\\.part-00000-0cb4d441-845a-4211-8ad3-b49a05ec763c-c000.snappy.parquet.crc]: it still exists.\n",
      "23/04/26 15:04:35 WARN FileUtil: Failed to delete file or dir [C:\\tmp\\firedataService_parquet\\files\\_temporary\\0\\_temporary\\attempt_20230426150434_0033_m_000000_811\\part-00000-0cb4d441-845a-4211-8ad3-b49a05ec763c-c000.snappy.parquet]: it still exists.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted.\u001b[39m\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m198\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m170\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\r\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m75\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m290\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\r\n  ammonite.$sess.cmd20$Helper.<init>(\u001b[32mcmd20.sc\u001b[39m:\u001b[32m1\u001b[39m)\r\n  ammonite.$sess.cmd20$.<init>(\u001b[32mcmd20.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd20$.<clinit>(\u001b[32mcmd20.sc\u001b[39m:\u001b[32m-1\u001b[39m)\r\n\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 811, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\firedataService_parquet\\files\\_temporary\\0\\_temporary\\attempt_20230426150434_0033_m_000000_811\\part-00000-0cb4d441-845a-4211-8ad3-b49a05ec763c-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\u001b[39m\r\n  org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1891\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1879\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1878\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\r\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1878\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m927\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m927\u001b[39m)\r\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m407\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m927\u001b[39m)\r\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2112\u001b[39m)\r\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2061\u001b[39m)\r\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2050\u001b[39m)\r\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m738\u001b[39m)\r\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m167\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m170\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\r\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m75\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m290\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\r\n  ammonite.$sess.cmd20$Helper.<init>(\u001b[32mcmd20.sc\u001b[39m:\u001b[32m1\u001b[39m)\r\n  ammonite.$sess.cmd20$.<init>(\u001b[32mcmd20.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd20$.<clinit>(\u001b[32mcmd20.sc\u001b[39m:\u001b[32m-1\u001b[39m)\r\n\u001b[31mjava.io.IOException: (null) entry in command string: null chmod 0644 C:\\tmp\\firedataService_parquet\\files\\_temporary\\0\\_temporary\\attempt_20230426150434_0033_m_000000_811\\part-00000-0cb4d441-845a-4211-8ad3-b49a05ec763c-c000.snappy.parquet\u001b[39m\r\n  org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(\u001b[32mShell.java\u001b[39m:\u001b[32m762\u001b[39m)\r\n  org.apache.hadoop.util.Shell.execCommand(\u001b[32mShell.java\u001b[39m:\u001b[32m859\u001b[39m)\r\n  org.apache.hadoop.util.Shell.execCommand(\u001b[32mShell.java\u001b[39m:\u001b[32m842\u001b[39m)\r\n  org.apache.hadoop.fs.RawLocalFileSystem.setPermission(\u001b[32mRawLocalFileSystem.java\u001b[39m:\u001b[32m661\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem$1.apply(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m501\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m482\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem.setPermission(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m498\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem.create(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m467\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem.create(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m433\u001b[39m)\r\n  org.apache.hadoop.fs.FileSystem.create(\u001b[32mFileSystem.java\u001b[39m:\u001b[32m908\u001b[39m)\r\n  org.apache.hadoop.fs.FileSystem.create(\u001b[32mFileSystem.java\u001b[39m:\u001b[32m889\u001b[39m)\r\n  org.apache.parquet.hadoop.util.HadoopOutputFile.create(\u001b[32mHadoopOutputFile.java\u001b[39m:\u001b[32m74\u001b[39m)\r\n  org.apache.parquet.hadoop.ParquetFileWriter.<init>(\u001b[32mParquetFileWriter.java\u001b[39m:\u001b[32m248\u001b[39m)\r\n  org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(\u001b[32mParquetOutputFormat.java\u001b[39m:\u001b[32m390\u001b[39m)\r\n  org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(\u001b[32mParquetOutputFormat.java\u001b[39m:\u001b[32m349\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(\u001b[32mParquetOutputWriter.scala\u001b[39m:\u001b[32m37\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(\u001b[32mParquetFileFormat.scala\u001b[39m:\u001b[32m151\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(\u001b[32mFileFormatDataWriter.scala\u001b[39m:\u001b[32m123\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(\u001b[32mFileFormatDataWriter.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m236\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m177\u001b[39m)\r\n  org.apache.spark.scheduler.ResultTask.runTask(\u001b[32mResultTask.scala\u001b[39m:\u001b[32m90\u001b[39m)\r\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m123\u001b[39m)\r\n  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m411\u001b[39m)\r\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\r\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m414\u001b[39m)\r\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mUnknown Source\u001b[39m)\r\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mUnknown Source\u001b[39m)\r\n  java.lang.Thread.run(\u001b[32mUnknown Source\u001b[39m)"
     ]
    }
   ],
   "source": [
    "data.write.format(\"parquet\").save(\"/tmp/firedataService_parquet/files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q18. Rechargez  les données stockées en format Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd22.sc:1: not found: value sparkSession\n",
      "val newdataDF = sparkSession.read.parquet(\"/tmp/firedataService_parquet/files\")\n",
      "                ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "val newdataDF = sparkSession.read.parquet(\"/tmp/firedataService_parquet/files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd21.sc:1: not found: value newdataDF\n",
      "val res21 = newdataDF.printSchema\n",
      "            ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "newdataDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
